{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPDPNBTCLFIHG228UFqeJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreesanthrnair/DSA_Notes/blob/main/ETL_vs_ELT_Building_Robust_Data_Pipelines_using_Apache_Airflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##  ETL vs ELT: Core Concepts\n",
        "\n",
        "| Aspect            | ETL (Extract, Transform, Load) | ELT (Extract, Load, Transform) |\n",
        "|------------------|--------------------------------|-------------------------------|\n",
        "| **Workflow Order** | Extract → Transform → Load     | Extract → Load → Transform    |\n",
        "| **Where Transformation Happens** | In a staging server or ETL tool | Inside the data warehouse (e.g., Snowflake, BigQuery) |\n",
        "| **Best For**      | Legacy systems, small/medium data | Cloud-native, big data platforms |\n",
        "| **Tools Used**    | Talend, Informatica, Apache NiFi | dbt, SQL scripts, Spark, BigQuery |\n",
        "| **Latency**       | Higher (batch-oriented)         | Lower (can be near real-time) |\n",
        "| **Flexibility**   | More control over transformation | More scalable and faster with modern warehouses |\n",
        "\n",
        "---\n",
        "\n",
        "##  ETL: Extract, Transform, Load\n",
        "\n",
        "- **Extract**: Pull data from sources (APIs, databases, files)\n",
        "- **Transform**: Clean, enrich, and reshape data (e.g., imputation, encoding)\n",
        "- **Load**: Push into target system (data warehouse or lake)\n",
        "\n",
        "###  Pros\n",
        "- Good for complex transformations\n",
        "- Works well with structured data\n",
        "\n",
        "###  Cons\n",
        "- Slower for large datasets\n",
        "- Requires intermediate storage\n",
        "\n",
        "---\n",
        "\n",
        "##  ELT: Extract, Load, Transform\n",
        "\n",
        "- **Extract**: Same as ETL\n",
        "- **Load**: Push raw data directly into warehouse\n",
        "- **Transform**: Use SQL or warehouse-native tools to process data\n",
        "\n",
        "###  Pros\n",
        "- Leverages warehouse compute power\n",
        "- Scales better with big data\n",
        "- Faster deployment and iteration\n",
        "\n",
        "###  Cons\n",
        "- Less control over transformation logic\n",
        "- Requires strong warehouse performance\n",
        "\n",
        "---\n",
        "\n",
        "##  Apache Airflow: Orchestrating Data Pipelines\n",
        "\n",
        "Apache Airflow is a powerful open-source tool for **workflow orchestration**. It lets you define, schedule, and monitor ETL/ELT pipelines as **DAGs (Directed Acyclic Graphs)**.\n",
        "\n",
        "###  Key Concepts\n",
        "\n",
        "| Term         | Description |\n",
        "|--------------|-------------|\n",
        "| **DAG**       | A pipeline defined as a graph of tasks |\n",
        "| **Task**      | A unit of work (e.g., extract, transform) |\n",
        "| **Operator**  | Predefined actions (e.g., PythonOperator, BashOperator, SQLOperator) |\n",
        "| **Scheduler** | Triggers DAGs based on time or events |\n",
        "| **Executor**  | Runs tasks in parallel or sequentially |\n",
        "\n",
        "---\n",
        "\n",
        "###  Building ETL/ELT Pipelines with Airflow\n",
        "\n",
        "#### Step-by-Step ETL Example:\n",
        "```python\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "def extract():\n",
        "    # Pull data from API or DB\n",
        "    pass\n",
        "\n",
        "def transform():\n",
        "    # Clean and enrich data\n",
        "    pass\n",
        "\n",
        "def load():\n",
        "    # Push to warehouse\n",
        "    pass\n",
        "\n",
        "with DAG('etl_pipeline', start_date=datetime(2023, 1, 1), schedule_interval='@daily') as dag:\n",
        "    t1 = PythonOperator(task_id='extract', python_callable=extract)\n",
        "    t2 = PythonOperator(task_id='transform', python_callable=transform)\n",
        "    t3 = PythonOperator(task_id='load', python_callable=load)\n",
        "\n",
        "    t1 >> t2 >> t3\n",
        "```\n",
        "\n",
        "#### ELT Variation:\n",
        "- Replace `transform()` with SQL scripts using `PostgresOperator` or `BigQueryOperator`\n",
        "- Load raw data first, then run transformation inside the warehouse\n",
        "\n",
        "---\n",
        "\n",
        "###  Monitoring & Scaling\n",
        "\n",
        "- Use Airflow UI to track DAG runs, logs, and task status\n",
        "- Integrate with cloud platforms (AWS, GCP, Azure)\n",
        "- Use sensors and hooks for dynamic workflows\n",
        "\n",
        "---\n",
        "\n",
        "##  Best Practices\n",
        "\n",
        "- Modularize your code (separate extract, transform, load logic)\n",
        "- Use environment variables and secrets managers\n",
        "- Implement retries and alerting for failures\n",
        "- Version control your DAGs (Git + CI/CD)\n",
        "- Document your pipeline logic clearly\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YybQlZkQS2zO"
      }
    }
  ]
}