{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ+pjokdoWX0zicj0v2cfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreesanthrnair/DSA_Notes/blob/main/Reading_data_from_various_sources%2CArray%2CDataframe%2Cvectors%2Cseries%2CIntroduction_to_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##  Reading Data from Various Sources\n",
        "\n",
        "Data can come from many places—files, databases, APIs, or distributed systems. Here's how to handle them:\n",
        "\n",
        "###  Common Sources & Tools\n",
        "\n",
        "| Source Type     | Tool/Library        | Method/Function |\n",
        "|-----------------|---------------------|------------------|\n",
        "| **CSV/Excel**    | `pandas`, `openpyxl` | `pd.read_csv()`, `pd.read_excel()` |\n",
        "| **SQL Databases**| `SQLAlchemy`, `sqlite3`, `pandas` | `pd.read_sql_query()` |\n",
        "| **JSON/XML**     | `json`, `xml.etree`, `pandas` | `pd.read_json()`, `ElementTree` |\n",
        "| **Web APIs**     | `requests`, `urllib` | `requests.get()`, `json.loads()` |\n",
        "| **Big Data**     | `PySpark`, `Dask`    | `spark.read.csv()`, `dask.read_csv()` |\n",
        "\n",
        "---\n",
        "\n",
        "##  Arrays, DataFrames, Vectors, Series\n",
        "\n",
        "Understanding these structures helps you manipulate and analyze data efficiently.\n",
        "\n",
        "###  1. Arrays (NumPy)\n",
        "- **Definition**: Homogeneous, multi-dimensional data structure\n",
        "- **Library**: `numpy`\n",
        "- **Use Case**: Mathematical operations, matrix manipulation\n",
        "```python\n",
        "import numpy as np\n",
        "arr = np.array([1, 2, 3])\n",
        "```\n",
        "\n",
        "###  2. Series (pandas)\n",
        "- **Definition**: One-dimensional labeled array\n",
        "- **Library**: `pandas`\n",
        "- **Use Case**: Time series, single column operations\n",
        "```python\n",
        "import pandas as pd\n",
        "s = pd.Series([10, 20, 30], index=['a', 'b', 'c'])\n",
        "```\n",
        "\n",
        "###  3. DataFrame (pandas)\n",
        "- **Definition**: Two-dimensional labeled data structure\n",
        "- **Library**: `pandas`\n",
        "- **Use Case**: Tabular data analysis, filtering, grouping\n",
        "```python\n",
        "df = pd.DataFrame({'Name': ['A', 'B'], 'Score': [85, 90]})\n",
        "```\n",
        "\n",
        "###  4. Vectors (MLlib in PySpark)\n",
        "- **Definition**: Dense or sparse vector used in machine learning\n",
        "- **Library**: `pyspark.ml.linalg`\n",
        "- **Use Case**: Feature representation for ML models\n",
        "```python\n",
        "from pyspark.ml.linalg import Vectors\n",
        "v = Vectors.dense([1.0, 0.0, 3.0])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "##  Introduction to PySpark\n",
        "\n",
        "PySpark is the Python API for Apache Spark—a distributed computing engine for big data processing.\n",
        "\n",
        "###  Why PySpark?\n",
        "- Handles **large-scale data** across clusters\n",
        "- Supports **SQL**, **streaming**, **ML**, and **graph processing**\n",
        "- Integrates with **Hadoop**, **Hive**, **Kafka**, and **Delta Lake**\n",
        "\n",
        "###  Core Concepts\n",
        "\n",
        "| Concept        | Description |\n",
        "|----------------|-------------|\n",
        "| **SparkSession** | Entry point to Spark functionality |\n",
        "| **RDD**          | Low-level resilient distributed dataset |\n",
        "| **DataFrame**    | High-level abstraction for structured data |\n",
        "| **Transformations** | Lazy operations (e.g., `filter`, `map`) |\n",
        "| **Actions**       | Trigger execution (e.g., `collect`, `show`) |\n",
        "\n",
        "###  Basic PySpark Workflow\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "\n",
        "# Read CSV\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show data\n",
        "df.show()\n",
        "\n",
        "# Filter and select\n",
        "df.select(\"column1\").filter(df[\"column2\"] > 50).show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "##  Best Practices\n",
        "\n",
        "- Use **pandas** for small to medium datasets\n",
        "- Switch to **PySpark** for distributed or large-scale data\n",
        "- Convert between pandas and PySpark using `toPandas()` and `spark.createDataFrame()`\n",
        "- Validate schema and types before transformations\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4IAf_laiUrl0"
      }
    }
  ]
}